### 《深入剖析 Kubernetes》学习笔记 Day 16

今天学习了容器编排与Kubernetes作业管理 (15讲)之「19 | 深入理解StatefulSet（二）：存储状态」。

#### 前文回顾

StatefulSet 如何保证应用实例的拓扑状态，在 Pod 删除和再创建的过程中保持稳定。

#### coredns pod不断重启

```
# kubectl get pods -n kube-system -owide

NAME                             READY     STATUS             RESTARTS   AGE       IP              NODE       NOMINATED NODE
coredns-54b65f9c9c-5v8gh         0/1       CrashLoopBackOff   232        6d        10.32.0.2       ubuntu     <none>
coredns-54b65f9c9c-86v6q         1/1       Running            233        6d        10.32.0.3       ubuntu     <none>
```

查看Events发现，Liveness probe failed。

```
# kubectl describe pod -n kube-system coredns-54b65f9c9c-5v8gh

Events:
  Type     Reason     Age                 From             Message
  ----     ------     ----                ----             -------
  Warning  Unhealthy  7m (x712 over 5d)   kubelet, ubuntu  Liveness probe failed: Get http://10.32.0.2:8080/health: net/http: HTTP/1.x transport connection broken: unexpected EOF
  Warning  BackOff    2m (x1674 over 5d)  kubelet, ubuntu  Back-off restarting failed container
```

查看logs发现，no route to host。

```
# kubectl logs -n kube-system coredns-54b65f9c9c-5v8gh

E0131 09:21:12.288092       1 reflector.go:205] github.com/coredns/coredns/plugin/kubernetes/controller.go:313: Failed to list *v1.Service: Get https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0: dial tcp 10.96.0.1:443: connect: no route to host
```

强制删除重建解决

```
kubectl delete pod -n kube-system coredns-54b65f9c9c-5v8gh --grace-period=0 --force
kubectl delete pod -n kube-system coredns-54b65f9c9c-86v6q --grace-period=0 --force
```

> 感悟：coredns pod 老是重启问题解决了，掌握了一点诊断问题的方法，kubectl describe查看Events报错，kubectl logs查看Pod日志。

#### k8s命令bash自动补全

```
# ubuntu
apt-get install bash-completion

# centos
yum install bash-completion

kubectl completion bash

# vim ~/.bashrc
if [ -f /etc/bash_completion ] && ! shopt -oq posix; then
    . /etc/bash_completion
fi
```

> 感悟：工欲善其事，必先利其器！

#### 深入理解StatefulSet（二）：存储状态

在 Pod 里可以使用 spec.volumes.rbd 声明 Ceph RBD类型的Volume，这样会将存储底层细节过度暴露，于是产生了 PVC 和 PV 的设计，用来解耦存储使用方和存储提供方。

**PVC：Persistent Volume Claim**

```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

**PV：Persistent Volume**

```
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume
  labels:
    type: local
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  rbd:
    monitors:
    # 使用 kubectl get pods -n rook-ceph 查看 rook-ceph-mon- 开头的 POD IP 即可得下面的列表
    - '10.16.154.78:6789'
    - '10.16.154.82:6789'
    - '10.16.154.83:6789'
    pool: kube
    image: foo
    fsType: ext4
    readOnly: true
    user: admin
    keyring: /etc/ceph/keyring
```

**Pod 使用 PVC**

```
apiVersion: v1
kind: Pod
metadata:
  name: pv-pod
spec:
  containers:
    - name: pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: pv-storage
  volumes:
    - name: pv-storage
      persistentVolumeClaim:
        claimName: pv-claim
```

**StatefulSet**

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
```

> 感悟：需要搭建一个ceph服务来实验。

**存储状态管理**

StatefulSet 控制器直接管理的是 Pod。每个 Pod 的 hostname、名字等都是不同的、携带了编号的。而 StatefulSet 区分这些实例的方式，就是通过在 Pod 的名字里加上事先约定好的编号。

通过 Headless Service，为这些有编号的 Pod，在 DNS 服务器中生成带有同样编号的 DNS 记录。只要 StatefulSet 能够保证这些 Pod 名字里的编号不变，那么 Service 里的 DNS 记录也就不会变，而这条记录解析出来的 Pod 的 IP 地址，则会随着后端 Pod 的删除和再创建而自动更新。这当然是 Service 机制本身的能力，不需要 StatefulSet 操心。

StatefulSet 还为每一个 Pod 分配并创建一个同样编号的 PVC。这样就可以通过 Persistent Volume 机制为这个 PVC 绑定上对应的 PV，从而保证了每一个 Pod 都拥有一个独立的 Volume。

Pod 被删除，它所对应的 PVC 和 PV 会保留下来。当这个 Pod 被重新创建出来之后，会为它找到同样编号的 PVC，挂载这个 PVC 对应的 Volume，从而访问之前保存在 Volume 里的数据。

> 感悟：StatefulSet的PVC是自动创建的，其生命周期比Pod要久。这个过程还需要再通过实验结合日志消化一下。

学习来源： 极客时间 https://time.geekbang.org/column/intro/100015201?tab=catalog

